{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9436224,"sourceType":"datasetVersion","datasetId":5700575}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kindly note that some of the files aren't composed into a new data set yet, as they were \n# downloaded using a beatuifulsoup from the Nasa Space Apps Challenge 2024 resources. \n# For more information on the project: https://github.com/Ahmed-Samir11/Cosmic-Analysts","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\n\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom datetime import datetime, timedelta\nfrom obspy import read\nfrom obspy import UTCDateTime\nfrom torchinfo import summary\nfrom utils import *\nfrom autoencoder_model import *\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.preprocessing import OneHotEncoder\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\n# Set the plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_context(\"talk\")\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explore an example of data","metadata":{}},{"cell_type":"code","source":"file_path = r\"xb.elyse.00.hhv.2019.066.1.mseed\" #File Manually downloaded from the full data archive\ntry:\n    stream = read(file_path)\n    print(\"File successfully read!\")\n    print(stream)\nexcept Exception as e:\n    print(f\"Error reading file with ObsPy: {e}\")\nfor tr in stream:\n    print(\"\\nTrace Statistics:\")\n    print(f\"Network: {tr.stats.network}\")\n    print(f\"Station: {tr.stats.station}\")\n    print(f\"Channel: {tr.stats.channel}\")\n    print(f\"Location: {tr.stats.location}\")\n    print(f\"Start Time: {tr.stats.starttime}\")\n    print(f\"End Time: {tr.stats.endtime}\")\n    print(f\"Sampling Rate: {tr.stats.sampling_rate} Hz\")\n    print(f\"Number of Samples: {tr.stats.npts}\")\n    print(f\"Data Type: {tr.data.dtype}\")\n    # If needed, you can also access the raw data\n    print(f\"Data: {tr.data}\")\n\n# Print general metadata\nprint(\"\\nGeneral Metadata:\")\nprint(f\"Number of Traces: {len(stream)}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trace = stream[0]\nplt.figure(figsize=(10, 6))\nplt.plot(trace.times(), trace.data)\nplt.xlabel('Time (s)')\nplt.ylabel('Velocity')\nplt.title('Seismic Trace')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"guided_df = pd.read_csv(\"Mars_InSight_training_catalog.csv\")\nguided_df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row = guided_df.iloc[0]\nrow","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"arrival_time = datetime.strptime(row['time'],'%Y-%m-%dT%H:%M:%S.%f')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mseed_file = r'C:\\Users\\ahmed\\OneDrive\\Desktop\\Cosmic Analysts\\XB.ELYSE.02.BHV.2022-02-03HR08_evid0005.mseed'\nst = read(mseed_file)\nprint(st)\nst[0].stats","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr = st.traces[0].copy()\ntr_times = tr.times()\ntr_data = tr.data\nstarttime = tr.stats.starttime.datetime\n\n# Create a vector for the absolute time\ntr_times_dt = []\nfor tr_val in tr_times:\n    tr_times_dt.append(starttime + timedelta(seconds=tr_val))\n\n# Plot the absolute result\nfig,ax = plt.subplots(1,1,figsize=(10,3))\n\n# Plot trace\nax.plot(tr_times_dt,tr_data)\n\n# Mark detection\narrival_line = ax.axvline(x=arrival_time, c='red', label='Abs. Arrival')\nax.legend(handles=[arrival_line])\n\n# Make the plot pretty\nax.set_xlim([min(tr_times_dt),max(tr_times_dt)])\nax.set_ylabel('Velocity (m/s)')\nax.set_xlabel('Time (s)')\nax.set_title('XB.ELYSE.02.BHV.2022-02-03HR08_evid0005.mseed', fontweight='bold')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mseed_file = r'xb.elyse.02.bhv.2022.034.7.mseed'\nst = read(mseed_file)\nprint(st)\nst[0].stats","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr = st.traces[0].copy()\ntr_times = tr.times()\ntr_data = tr.data\nstarttime = tr.stats.starttime.datetime\nendtime = tr.stats.endtime.datetime\n# Create a vector for the absolute time\ntr_times_dt = []\nfor tr_val in tr_times:\n    tr_times_dt.append(starttime + timedelta(seconds=tr_val))\n\n# Plot the absolute result\nfig,ax = plt.subplots(1,1,figsize=(10,3))\n\n# Plot trace\nax.plot(tr_times_dt,tr_data)\n\n# Mark detection\narrival_line = ax.axvline(x=arrival_time, c='red', label='Abs. Arrival')\nax.legend(handles=[arrival_line])\n\n# Make the plot pretty\nax.set_xlim([min(tr_times_dt),max(tr_times_dt)])\nax.set_ylabel('Velocity (m/s)')\nax.set_xlabel('Time (s)')\nax.set_title('XB.ELYSE.02.BHV.2022-02-03HR08_evid0005.mseed', fontweight='bold')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make DataFrame","metadata":{}},{"cell_type":"markdown","source":"### Map each location to its meaning and measurements","metadata":{}},{"cell_type":"markdown","source":"How files are named?\nMini-SEED data file:  [network].[station].[location].[channel].[year].[doy].[rev].mseed\n","metadata":{}},{"cell_type":"markdown","source":"Further Explanation: \\\nThe code is made up of 3 letters: Instrument code, Band code, and orientation code. ","metadata":{}},{"cell_type":"code","source":"instrument_code = {'H': 'High Gain Seismometer',\n                   'L': 'Low Gain Seismometer',\n                   'M': 'Mass Position Seismometer',\n                   'D': 'Pressure',\n                   'F': 'Magnetometer',\n                   'k': 'Temperature',\n                   'W': 'Wind',\n                   'Z': 'Sythetized Beam Data',\n                   'Y': 'Non-specific Instruments',\n                   'E': 'Electric Test Point'\n                   }","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"band_code = { 'SP' : {\n    'E': 100,\n    'S': [10, 80],\n    'M': [2, 5],\n    'L': 1,\n    'V': [0.1 , 0.5],\n    'U': [0.01, 0.05],\n    'R': [1/3600, 0.001]\n}, \n'VBB': {\n    'H': 100,\n    'B': [10, 80],\n    'M': [2, 5],\n    'L': 1,\n    'V': [0.1 , 0.5],\n    'U': [0.01, 0.05],\n    'R': [1/3600, 0.001]\n}\n}\n# List elements are range, not discrete values","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SP: Short Period Seismometer \\\nVBB: Very Broadband Seismometer \\\nAPPS: Auxiliary Payload Sensor Suite","metadata":{}},{"cell_type":"code","source":"# Define the dictionary\nloc_id = {\n    'SEIS/VBB related data': {\n        'Science': { \n            'High G.': {\n                'VBB': '00',\n                'Replaced SP': '20', # Synthesized SP  (from VBB1, VBB2, VBB3)\n                'spare': None, \n                'VBB RMS': '40', # High pass RMS over one second\n                'MAX VBB RMS': '45', # Maximum RMS over N seconds\n                'spare': '50'\n                }, \n            'Low G.': {\n                'VBB': '05',\n                'Replaced SP': None,\n                'spare': None,\n                'VBB RMS': None,\n                'MAX VBB RMS': None,\n                'spare': None\n            }\n            \n        },\n        'Engin.': {\n            'High G.': {\n                'VBB': '10',\n                'Replaced SP': None,\n                'spare': '30', # spare Ids for possible VBB open loop mode\n                'VBB RMS': None,\n                'MAX VBB RMS': None,\n                'spare': None\n                }, \n            'Low G.': {\n                'VBB': '15',\n                'Replaced SP': None,\n                'spare': '35', # spare Ids for possible VBB open loop mode\n                'VBB RMS': None,\n                'MAX VBB RMS': None,\n                'spare': None\n            }\n        }\n    },\n    'SEIS/Hybrid': { # Hybrid channels\n        'Science': {\n            'High G.': {\n                'VBB+SP': '55',\n                'spare': '60'\n                }, \n            'Low G.': {\n                'VBB+SP': None,\n                'spare': None\n            }\n        },\n        'Engin.': {\n            'High G.': {\n                'VBB+SP': None,\n                'spare': None\n                }, \n            'Low G.': {\n                'VBB+SP': None,\n                'spare': None\n            }\n        }\n    },\n    'SEIS/SP related data': {\n        'Science': {\n            'High G.': {\n                'SP': '65',\n                'Rotated SP': '75', # On board rotated SP (from SP1, SP2, SP3)\n                'Replaced VBB': '80', # Synthesized VBB (from SP1, SP2, SP3)\n                'SP RMS': '85', # High pass RMS over one second\n                'MAX SP RMS': '90', # Maximum RMS over N seconds\n                'spare': '95'\n                }, \n            'Low G.': {\n                'SP': '70',\n                'Rotated SP': None,\n                'Replaced VBB': None,\n                'SP RMS': None,\n                'MAX SP RMS': None,\n                'spare': None\n            }\n        },\n        'Engin.': {\n            'High G.': {\n                'SP': None,\n                'Rotated SP': None,\n                'Replaced VBB': None,\n                'SP RMS': None,\n                'MAX SP RMS': None,\n                'spare': None\n                }, \n            'Low G.': {\n                'SP': None,\n                'Rotated SP': None,\n                'Replaced VBB': None,\n                'SP RMS': None,\n                'MAX SP RMS': None,\n                'spare': None\n            }\n        }\n    },\n    'APSS related data': {\n        'Science': {\n            'High G.': {\n                'TWINS proc 1': '00', # Magnetometer, Pressure, temperature (raw data)\n                'TWINS Proc 2': '10', # On Earth Processed Data: wind amplitude and direction, atmospheric temperature\n                'Rotated MAG': '20', # On board rotated MAG (from mag1, mag2, mag3)\n                'MAG RMS': '30', # High pass RMS over one second\n                'MAX MAG RMS': '40', # Maximum RMS over N seconds\n                'P1 RMS': '50', # High pass RMS over one second\n                'P2 RMS': '60',\n                'MAX P1 RMS': '70', # Maximum RMS over N seconds\n                'MAX P2 RMS': '80',\n                'spare': '90'\n                }, \n            'Low G.': {\n                'TWINS proc 1': None,\n                'TWINS Proc 2': None,\n                'Rotated MAG': None,\n                'MAG RMS': None,\n                'MAX MAG RMS': None,\n                'P1 RMS': None,\n                'P2 RMS': None,\n                'MAX P1 RMS': None,\n                'MAX P2 RMS': None,\n                'spare': None\n            }\n        },\n        'Engin.': {\n            'High G.': {\n                'TWINS proc 1': None,\n                'TWINS Proc 2': None,\n                'Rotated MAG': None,\n                'MAG RMS': None,\n                'MAX MAG RMS': None,\n                'P1 RMS': None,\n                'P2 RMS': None,\n                'MAX P1 RMS': None,\n                'MAX P2 RMS': None,\n                'spare': None\n                }, \n            'Low G.': {\n                'TWINS proc 1': None,\n                'TWINS Proc 2': None,\n                'Rotated MAG': None,\n                'MAG RMS': None,\n                'MAX MAG RMS': None,\n                'P1 RMS': None,\n                'P2 RMS': None,\n                'MAX P1 RMS': None,\n                'MAX P2 RMS': None,\n                'spare': None\n            }\n        }\n    }\n}\n\ndf_list = []\nfor category, data in loc_id.items():\n    for sub_category, values in data.items():\n        df = pd.DataFrame(values).T\n        df.insert(0, 'Category', category)\n        df.insert(1, 'Sub-Category', sub_category)\n        df_list.append(df)\n\n#df_combined = pd.concat(df_list, ignore_index=True)\n#df_combined.head()\ndf_list[1]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"locid = pd.read_csv('all_loc_id.csv', dtype = {'LocID Root': 'object', 'LocID': 'object'})\nlocid.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l = ['Ancillary data', 'VBB Seismometer Channels', 'APSS Channels Index', 'SP Seismometer Channels', 'SEIS Software Synthesized Data' ]\nlocid = locid.drop([0] + locid.index[locid['Channel'].isin(l)].tolist())\nlocid['LocID Root'].astype('object')\nlocid.ffill(inplace=True)\nlocid.rename(columns={'Unnamed: 3' : 'Band'}, inplace=True)\nlocid.head() ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"locid.to_csv('all_locations_ids.csv', index_label=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List to hold features for all files (initially empty if not starting fresh)\nfeatures_list = []\nfolder_path = r'C:\\Users\\ahmed\\OneDrive\\Desktop\\Cosmic Analysts\\continuous_waveform'\n# Loop over folder and extract features\n'''\nfor filename in os.listdir(folder_path):\n    if filename.endswith(\".mseed\"):\n        file_path = os.path.join(folder_path, filename)\n        stream = read(file_path)\n            \n        # Extract features for each trace in the file\n        for tr in stream:\n            features = extract_features(tr, filename)\n            features_list.append(features)\n#features_df = pd.DataFrame(features_list)\n'''\nfeatures_df = pd.read_csv('Mars_InSight_Lander.csv', index_col=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"features_df.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_df.drop(columns=['start_time','end_time']).nunique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert obspy's UTCDateTime objects to Python datetime objects\nfeatures_df['start_time'] = features_df['start_time'].apply(lambda x: x.datetime if isinstance(x, UTCDateTime) else x)\nfeatures_df['end_time'] = features_df['end_time'].apply(lambda x: x.datetime if isinstance(x, UTCDateTime) else x)\nfeatures_df['start_time'] = pd.to_datetime(features_df['start_time'])\nfeatures_df['end_time'] = pd.to_datetime(features_df['end_time'])\nfeatures_df['year'] = features_df['start_time'].dt.year\nfeatures_df['duration_seconds'] = (features_df['end_time'] - features_df['start_time']).dt.total_seconds()\nfeatures_df['start_hour'] = features_df['start_time'].dt.hour\nfeatures_df['end_hour'] = features_df['end_time'].dt.hour","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plot features and find whether they are Gaussian or not.\nUnderstand the reason behind some features being NaN.\nAdd time stamp as a feature to the CSV.","metadata":{}},{"cell_type":"code","source":"features_df.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_df.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_df['channel'].unique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_df['location'].unique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_df.to_csv('Mars_InSight_Lander.csv', index=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = features_df\nloc_cha_to_channel = locid.set_index('locID.CHA')['Channel'].to_dict()\n\n# Map the locID.CHA in df2 to the Channels from df1\ndf2['Channels'] = df2['locID.CHA'].map(loc_cha_to_channel)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"channel_counts = df2['Channels'].value_counts()\ntop_channels = channel_counts.head(15)\nplt.figure(figsize=(12, 8))\ntop_channels.plot(kind='bar', edgecolor='black')\n\nplt.xticks(rotation=45, fontsize=10)\n\nplt.title('Top 30 Channels by Frequency')\nplt.xlabel('Channels')\nplt.ylabel('Frequency')\n\nplt.tight_layout() \nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering using Fast Fourier Transform","metadata":{}},{"cell_type":"code","source":"channels_for_feature_engineering = df2[df2['channel'].str[1].isin(['H', 'L'])]\nchannels_for_feature_engineering\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"channels_for_feature_engineering['channel'].nunique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nengineered_features_list = []\nfolder_path = r'C:\\Users\\ahmed\\OneDrive\\Desktop\\Cosmic Analysts\\continuous_waveform'\nfor i in channels_for_feature_engineering['file_name']:\n    file_path = os.path.join(folder_path, i)\n    stream = read(file_path)\n    #print(stream)\n    for tr in stream:\n        if tr.stats.channel in channels_for_feature_engineering['channel'].unique():\n            #print(tr.stats.channel)\n            features = feature_engineering(tr, i)\n            engineered_features_list.append(features)\n        else:\n            print(f'Trace{tr} of file {i} not included')\nengineered_features = pd.DataFrame(engineered_features_list)\nprint(engineered_features)\nprint(engineered_features['channel'].unique())\n# Convert obspy's UTCDateTime objects to Python datetime objects\nengineered_features['start_time'] = engineered_features['start_time'].apply(lambda x: x.datetime if isinstance(x, UTCDateTime) else x)\nengineered_features['end_time'] = engineered_features['end_time'].apply(lambda x: x.datetime if isinstance(x, UTCDateTime) else x)\nengineered_features['start_time'] = pd.to_datetime(engineered_features['start_time'])\nengineered_features['end_time'] = pd.to_datetime(engineered_features['end_time'])\nengineered_features['year'] = engineered_features['start_time'].dt.year\nengineered_features['duration_seconds'] = (engineered_features['end_time'] - engineered_features['start_time']).dt.total_seconds()\nengineered_features['start_hour'] = engineered_features['start_time'].dt.hour\nengineered_features['end_hour'] = engineered_features['end_time'].dt.hour\nengineered_features.to_csv('engineered_features_of_velocity_seismometer.csv', index=False)\n'''","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"engineered_features = pd.read_csv('engineered_features_of_velocity_seismometer.csv', index_col=False)\nengineered_features['start_time'] = engineered_features['start_time'].apply(lambda x: x.datetime if isinstance(x, UTCDateTime) else x)\nengineered_features['end_time'] = engineered_features['end_time'].apply(lambda x: x.datetime if isinstance(x, UTCDateTime) else x)\nengineered_features['start_time'] = pd.to_datetime(engineered_features['start_time'])\nengineered_features['end_time'] = pd.to_datetime(engineered_features['end_time'])\n\nengineered_features","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plotting various graphs based on different columns","metadata":{}},{"cell_type":"code","source":"engineered_features.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"engineered_features[engineered_features['skewness'].isnull()]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"85.LLZ is a transmitted processed data. ESTASP stands for Enhanced Seismic Transient Analysis for Surface Processes, a technique related to seismic data analysis in the context of planetary exploration. It was used in NASA's InSight mission to better understand seismic activities and surface processes on Mars.","metadata":{}},{"cell_type":"code","source":"engineered_features.dropna(inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_columns = engineered_features.select_dtypes(include=['float32', 'int32', 'float64', 'float32']).columns\n\n# Plot histograms for each numerical column\nengineered_features[numerical_columns].hist(figsize=(12, 10), bins=30, edgecolor='black')\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Sampling Rate Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(engineered_features['sampling_rate'], bins=50, kde=True)\nplt.title('Distribution of Sampling Rates')\nplt.xlabel('Sampling Rate')\nplt.ylabel('Frequency')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Scatter Plot: Mean vs Standard Deviation (std)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.scatter(engineered_features['mean'], engineered_features['std'], alpha=0.5)\nplt.title('Mean vs. Standard Deviation (std)')\nplt.xlabel('Mean')\nplt.ylabel('Standard Deviation (std)')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Correlation Heatmap","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(24, 16))\ncorr = engineered_features[numerical_columns].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation Heatmap of Features')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Boxplot of energy by Location","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(21, 9))\nsns.boxplot(x='location', y='energy', data=engineered_features)\nplt.title('Boxplot of Energy by Location')\nplt.xlabel('Location')\nplt.ylabel('Energy')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Line Plot: Duration over time","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\nengineered_features.set_index('start_time')['duration_seconds'].resample('D').mean().plot()\nplt.title('Average Duration over Time (Daily)')\nplt.ylabel('Duration (Seconds)')\nplt.xlabel('Time')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Pairplot of Selected Features\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.scatter(engineered_features['fft_mean'], engineered_features['fft_std'], alpha=0.5)\nplt.title('fft_mean vs. fft_std')\nplt.xlabel('fft_mean')\nplt.ylabel('fft_std')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.scatter(engineered_features['skewness'], engineered_features['kurtosis'], alpha=0.5)\nplt.title('skewness vs. kurtosis')\nplt.xlabel('skewness')\nplt.ylabel('kurtosis')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#'mean', 'std', 'skewness', 'kurtosis', \nsns.pairplot(engineered_features[['mean', 'std', 'skewness']])\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.scatter(engineered_features['duration_seconds'], engineered_features['energy'], alpha=0.5)\nplt.title('duration_seconds vs. energy')\nplt.xlabel('duration_seconds')\nplt.ylabel('energy')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Bar Plot of Start Hour Counts","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.countplot(x='start_hour', data=engineered_features)\nplt.title('Count of Records by Start Hour')\nplt.xlabel('Start Hour')\nplt.ylabel('Count')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Violin Plot of FFT Std by Station","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 6))\nsns.violinplot(x='station', y='fft_std', data=engineered_features)\nplt.title('Distribution of FFT Std by Station')\nplt.xlabel('Station')\nplt.ylabel('FFT Std')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Histogram of Skewness and Kurtosis","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(20, 6))\nplt.hist([engineered_features['skewness'].dropna(), engineered_features['kurtosis'].dropna()], bins=50, label=['Skewness', 'Kurtosis'])\nplt.title('Histogram of Skewness and Kurtosis')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Joint Plot: Energy vs Duration","metadata":{}},{"cell_type":"code","source":"#sns.jointplot(x='energy', y='duration_seconds', data=features_df, kind='hex', gridsize=30)\n#plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and DataLoader initialization for Autoencoder","metadata":{}},{"cell_type":"code","source":"cols_to_use = ['locID.CHA','station', 'channel', 'location' , 'sampling_rate', 'num_of_samples',\n                'mean', 'std', 'skewness', 'kurtosis', 'fft_mean', 'fft_std', 'energy', 'year',\n                  'duration_seconds', 'start_hour', 'end_hour']\ncols_to_encode = ['locID.CHA','station', 'channel', 'location' , 'sampling_rate', 'year']\nle = LabelEncoder()\nfor i in cols_to_encode:\n    engineered_features[i] = le.fit_transform(engineered_features[i])\n# Preprocessing\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(engineered_features[cols_to_use].values)\nX_train, X_temp = train_test_split(X_scaled, test_size=0.15, random_state=seed, shuffle=True)\nX_test, X_val = train_test_split(X_temp, test_size=1/3, random_state=seed, shuffle=False)\n\n# Convert the feature data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n# DataLoader for batch processing\ntrain_dataset = TensorDataset(X_train_tensor, X_train_tensor)\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataset = TensorDataset(X_val_tensor, X_val_tensor)\nval_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_dataset = TensorDataset(X_test_tensor, X_test_tensor)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n\n#X_train_tensor = torch.tensor(engineered_features[cols_to_use].values)\n#train_dataset = TensorDataset(X_train_tensor, X_train_tensor)\n#train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the model\nmodel = Autoencoder(X_train_tensor.shape[1])\nprint(summary(model))\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_results = train_autoencoder(model, train_dataloader, val_dataloader, num_epochs=10, num_eval_epoch=1, patience=10 ,optimizer=optimizer, scheduler=scheduler, save_dir=r\"C:\\Users\\ahmed\\OneDrive\\Desktop\\Cosmic Analysts\\checkpoints\")\nreconstruction_errors = train_results['val_reconstruction_errors']\nreconstruction_errors","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_results = evaluate_autoencoder(model, test_dataloader,criterion=nn.MSELoss(), device='cpu')\ntest_reconstruction_errors = test_results[\"val_reconstruction_error\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#file_path = r'C:\\Users\\ahmed\\OneDrive\\Desktop\\Cosmic Analysts\\XB.ELYSE.02.BHV.2022-02-03HR08_evid0005.mseed'\n#process_seismic_data(model, file_path, threshold)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Plot loss over epochs\nplt.figure(figsize=(20, 6))\n\n# Plot training and validation loss over epochs\nplt.subplot(1, 3, 1)\nepochs_range = range(1, len(train_results[\"train_loss\"]) + 1)\nplt.plot(epochs_range, train_results[\"train_loss\"], label='Train Loss', marker='o')\nplt.plot(epochs_range, train_results[\"val_loss\"], label='Validation Loss', marker='x')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train and Validation Loss Over Epochs')\nplt.legend()\n\n# Plot validation reconstruction error with threshold\nplt.subplot(1, 3, 2)\nval_reconstruction_errors_flat = [item for sublist in train_results[\"val_reconstruction_errors\"] for item in sublist]\nthreshold = np.percentile(val_reconstruction_errors_flat, 99)  \n\nplt.hist(val_reconstruction_errors_flat, bins=50, alpha=0.7, label='Validation Reconstruction Error')\nplt.axvline(threshold, color='r', linestyle='dashed', linewidth=1, label='Threshold (95th Percentile)')\nplt.xlabel('Reconstruction Error')\nplt.ylabel('Frequency')\nplt.title('Validation Error Distribution')\nplt.legend()\n\n# Plot test reconstruction error with threshold\nplt.subplot(1, 3, 3)\nplt.hist(test_reconstruction_errors, bins=50, alpha=0.7, label='Test Reconstruction Error')\nplt.axvline(threshold, color='r', linestyle='dashed', linewidth=1, label='Threshold (95th Percentile)')\nplt.xlabel('Reconstruction Error')\nplt.ylabel('Frequency')\nplt.title('Test Error Distribution')\nplt.legend()\n\n# Plot validation anomalies count over epochs\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(train_results[\"val_anomalies_counts\"]) + 1), train_results[\"val_anomalies_counts\"], marker='o', color='b', label='Validation Anomalies Count')\nplt.xlabel('Epoch')\nplt.ylabel('Number of Anomalies')\nplt.title('Validation Anomalies Count Over Epochs')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_results[\"val_anomalies_counts\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_results[\"val_anomalies_count\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Unsupervised ML","metadata":{}},{"cell_type":"markdown","source":"## Clustering","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('engineered_features_of_velocity_seismometer.csv', index_col=False)\ndf['start_time'] = pd.to_datetime(df['start_time'])\ndf['end_time'] = pd.to_datetime(df['end_time'])\ncols_to_use = ['locID.CHA','station', 'channel', 'location' , 'sampling_rate', 'num_of_samples',\n                'mean', 'std', 'skewness', 'kurtosis', 'fft_mean', 'fft_std', 'energy', 'year',\n                  'duration_seconds', 'start_hour', 'end_hour']\ncols_to_encode = ['locID.CHA','station', 'channel', 'location' , 'sampling_rate', 'year']\nle = LabelEncoder()\nfor i in cols_to_encode:\n    df[i] = le.fit_transform(df[i])\nprint(df.info())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = df[cols_to_use]\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data)\ndata = pd.DataFrame(np_scaled)\n# reduce to 2 importants features\npca = PCA(n_components=2)\ndata = pca.fit_transform(data)\n# standardize these 2 new features\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data)\ndata = pd.DataFrame(np_scaled)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# calculate with different number of centroids to see the loss plot (elbow method)\nn_cluster = range(1, 20)\nkmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\nscores = [kmeans[i].score(data) for i in range(len(kmeans))]\nfig, ax = plt.subplots()\nax.plot(n_cluster, scores)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I choose 14 centroids arbitrarily and add these data to the central dataframe\ndf['cluster'] = kmeans[14].predict(data)\ndf['principal_feature1'] = data[0]\ndf['principal_feature2'] = data[1]\ndf['cluster'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plot the different clusters with the 2 main features\nfig, ax = plt.subplots()\ncolors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'orange', 6:'cyan', 7:'yellow', 8:'brown', 9:'purple', 10:'white', 11: 'grey', 12:'lightblue', 13:'lightgreen', 14: 'darkgrey'}\nax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"cluster\"].apply(lambda x: colors[x]))\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\ndef getDistanceByPoint(data, model):\n    distance = pd.Series()\n    for i in range(0,len(data)):\n        Xa = np.array(data.loc[i])\n        Xb = model.cluster_centers_[model.labels_[i]-1]\n        distance.at[i] = np.linalg.norm(Xa - Xb)\n    return distance\noutliers_fraction = 0.01\ndistance = getDistanceByPoint(data, kmeans[14])\nnumber_of_outliers = int(outliers_fraction*len(distance))\nthreshold = distance.nlargest(number_of_outliers).min()\n# anomaly21 contain the anomaly result of method 2.1 Cluster (0:normal, 1:anomaly) \ndf['anomaly21'] = (distance >= threshold).astype(int)\nprint(df['anomaly21'].value_counts())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.dates as mdates\n# Select anomaly data\na = df.loc[df['anomaly21'] == 1, ['start_time', 'duration_seconds']]\n\nfig, ax = plt.subplots(figsize=(30, 6))\n\n# Convert datetime64 to numerical format using mdates.date2num\na['start_time_num'] = mdates.date2num(a['start_time'])\n\n# Resample and plot the average duration per day\ndf.set_index('start_time')['duration_seconds'].resample('D').mean().plot(ax=ax)\n\n# Plot anomalies using the converted numerical dates\nax.scatter(a['start_time_num'], a['duration_seconds'], color='red')\n\n# Simplify date formatting on x-axis\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\nax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=6))  # Show labels only for every nth day (change interval as needed)\n\n# Rotate date labels for better visibility\nplt.xticks(rotation=45)\n\n# Add x-axis label instead of individual dates\nplt.xlabel('Date')\n\nplt.show()\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Isolation Forest","metadata":{}},{"cell_type":"code","source":"data = df[cols_to_use]\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data)\ndata = pd.DataFrame(np_scaled)\n# train isolation forest \nmodel =  IsolationForest(contamination = outliers_fraction)\nmodel.fit(data)\n# add the data to the main  \ndf['anomaly25'] = pd.Series(model.predict(data))\ndf['anomaly25'] = df['anomaly25'].map( {1: 0, -1: 1} )\nprint(df['anomaly25'].value_counts())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = df.loc[df['anomaly25'] == 1, ['start_time', 'duration_seconds']]\n# Convert datetime64 to numerical format using mdates.date2num\na['start_time_num'] = mdates.date2num(a['start_time'])\n\nfig, ax = plt.subplots(figsize=(30, 6))\n\n# Convert datetime64 to numerical format using mdates.date2num\na['start_time_num'] = mdates.date2num(a['start_time'])\n\n# Resample and plot the average duration per day\ndf.set_index('start_time')['duration_seconds'].resample('D').mean().plot(ax=ax)\n\n# Plot anomalies using the converted numerical dates\nax.scatter(a['start_time_num'], a['duration_seconds'], color='red')\n\n# Simplify date formatting on x-axis\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\nax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=6))  # Show labels only for every nth day (change interval as needed)\n\n# Rotate date labels for better visibility\nplt.xticks(rotation=45)\n\n# Add x-axis label instead of individual dates\nplt.xlabel('Date')\n\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = r'C:\\Users\\ahmed\\OneDrive\\Desktop\\Cosmic Analysts\\XB.ELYSE.02.BHV.2022-02-03HR08_evid0005.mseed'\nst = read(file_path)\ntr = st[0]  # Assuming only one trace for simplicity\n\n# Extract data and timestamps\ntr_data = tr.data\ntr_times = np.linspace(0, len(tr_data) / tr.stats.sampling_rate, num=len(tr_data))\n\nplt.figure(figsize=(12, 4))\nplt.plot(tr_times, tr_data)\nplt.title('Seismic Trace')\nplt.xlabel('Time (s)')\nplt.ylabel('Velocity')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize the data (optional)\ntr_data_norm = (tr_data - np.mean(tr_data)) / np.std(tr_data)\n\n# Reshape the data for isolation forest (assuming each sample is a separate point)\ndata_points = tr_data_norm.reshape(-1, 1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Isolation Forest\niso_forest = IsolationForest(contamination=0.01, random_state=42)\n\n# Fit the model on seismic data\niso_forest.fit(data_points)\n\n# Predict anomalies\nanomalies = iso_forest.predict(data_points)\n\n# Label anomalies as -1 and normal data as 1\nanomalies_indices = np.where(anomalies == -1)[0]\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.plot(tr_times, tr_data_norm, label='Seismic Data')\nplt.scatter(tr_times[anomalies_indices], tr_data_norm[anomalies_indices], color='red', label='Detected Events', marker='x')\nplt.title('Seismic Event Detection Using Isolation Forest')\nplt.xlabel('Time (s)')\nplt.ylabel('Normalized Velocity')\nplt.legend()\nplt.show()\n\n# Optionally, print the times where anomalies were detected\nevent_times = tr_times[anomalies_indices]\nprint(\"Seismic events detected at times (s):\", event_times)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = r'C:\\Users\\ahmed\\OneDrive\\Desktop\\Cosmic Analysts\\XB.ELYSE.02.BHV.2022-02-03HR08_evid0005.mseed'\nst = read(file_path)\ntr = st[0]  # Assuming only one trace for simplicity\n\n# Extract data and timestamps\ntr_data = tr.data\ntr_times = np.linspace(0, len(tr_data) / tr.stats.sampling_rate, num=len(tr_data))\n\n# Normalize the data (optional)\ntr_data_norm = (tr_data - np.mean(tr_data)) / np.std(tr_data)\n\n# Reshape the data for isolation forest (assuming each sample is a separate point)\ndata_points = tr_data_norm.reshape(-1, 1)\n\n# Initialize Isolation Forest\niso_forest = IsolationForest(contamination=0.0007, random_state=42)\n\n# Fit the model on seismic data\niso_forest.fit(data_points)\n\n# Predict anomalies\nanomalies = iso_forest.predict(data_points)\n\n# Label anomalies as -1 and normal data as 1\nanomalies_indices = np.where(anomalies == -1)[0]\n\n# Threshold for event separation (in seconds)\nmin_separation_time = 1  # adjust this value based on your data's timescale\nlast_event_time = -np.inf  # initialize to negative infinity\n\nfiltered_anomalies_indices = []\nfor idx in anomalies_indices:\n    if tr_times[idx] - last_event_time > min_separation_time:\n        filtered_anomalies_indices.append(idx)\n        last_event_time = tr_times[idx]  # update the last event time\n\nfiltered_anomalies_indices = np.array(filtered_anomalies_indices)\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(tr_times, tr_data_norm, label='Seismic Data')\nplt.scatter(tr_times[filtered_anomalies_indices], tr_data_norm[filtered_anomalies_indices], color='red', label='Detected Events', marker='x')\nplt.title('Seismic Event Detection with Time-Based Separation')\nplt.xlabel('Time (s)')\nplt.ylabel('Normalized Velocity')\nplt.legend()\nplt.show()\n\n# Optionally, print the times where filtered anomalies were detected\nevent_times = tr_times[filtered_anomalies_indices]\nprint(\"Filtered seismic events detected at times (s):\", event_times)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from obspy.signal.invsim import cosine_taper\nfrom obspy.signal.filter import highpass\nfrom obspy.signal.trigger import classic_sta_lta, plot_trigger, trigger_onset\n\n# Sampling frequency of our trace\ndf = tr.stats.sampling_rate\n\n# How long should the short-term and long-term window be, in seconds?\nsta_len = 120\nlta_len = 600\n\n# Run Obspy's STA/LTA to obtain a characteristic function\n# This function basically calculates the ratio of amplitude between the short-term \n# and long-term windows, moving consecutively in time across the data\ncft = classic_sta_lta(tr_data, int(sta_len * df), int(lta_len * df))\n\n# Plot characteristic function\nfig,ax = plt.subplots(1,1,figsize=(12,3))\nax.plot(tr_times,cft)\nax.set_xlim([min(tr_times),max(tr_times)])\nax.set_xlabel('Time (s)')\nax.set_ylabel('Characteristic function')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Play around with the on and off triggers, based on values in the characteristic function\nthr_on = 4\nthr_off = 1.5\non_off = np.array(trigger_onset(cft, thr_on, thr_off))\n# The first column contains the indices where the trigger is turned \"on\". \n# The second column contains the indices where the trigger is turned \"off\".\n\n# Plot on and off triggers\nfig,ax = plt.subplots(1,1,figsize=(15,6))\nfor i in np.arange(0,len(on_off)):\n    triggers = on_off[i]\n    ax.axvline(x = tr_times[triggers[0]], color='red', label='Trig. On')\n    print(f'Event detected at {tr_times[triggers[0]]}')\n    ax.axvline(x = tr_times[triggers[1]], color='purple', label='Trig. Off')\n\n# Plot seismogram\nax.plot(tr_times,tr_data)\nax.set_xlim([min(tr_times),max(tr_times)])\nax.legend()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## One Class SVM","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('engineered_features_of_velocity_seismometer.csv', index_col=False)\ndf['start_time'] = pd.to_datetime(df['start_time'])\ndf['end_time'] = pd.to_datetime(df['end_time'])\ncols_to_use = ['locID.CHA','station', 'channel', 'location' , 'sampling_rate', 'num_of_samples',\n                'mean', 'std', 'skewness', 'kurtosis', 'fft_mean', 'fft_std', 'energy', 'year',\n                  'duration_seconds', 'start_hour', 'end_hour']\ncols_to_encode = ['locID.CHA','station', 'channel', 'location' , 'sampling_rate', 'year']\nle = LabelEncoder()\nfor i in cols_to_encode:\n    df[i] = le.fit_transform(df[i])\ndf.dropna(inplace=True)\ndata = df[cols_to_use]\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data)\ndata = pd.DataFrame(np_scaled)\n# train one class SVM \nmodel =  OneClassSVM(nu=0.95 * outliers_fraction) #nu=0.95 * outliers_fraction  + 0.05\ndata = pd.DataFrame(np_scaled)\nmodel.fit(data)\n# add the data to the main  \ndf['anomaly26'] = pd.Series(model.predict(data))\ndf['anomaly26'] = df['anomaly26'].map( {1: 0, -1: 1} )\nprint(df['anomaly26'].value_counts())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = df.loc[df['anomaly26'] == 1, ['start_time', 'duration_seconds']]\n# Convert datetime64 to numerical format using mdates.date2num\na['start_time_num'] = mdates.date2num(a['start_time'])\n\nfig, ax = plt.subplots(figsize=(30,6))\n\n# Convert datetime64 to numerical format using mdates.date2num\na['start_time_num'] = mdates.date2num(a['start_time'])\n\n# Resample and plot the average duration per day\ndf.set_index('start_time')['duration_seconds'].resample('D').mean().plot(ax=ax)\n\n# Plot anomalies using the converted numerical dates\nax.scatter(a['start_time_num'], a['duration_seconds'], color='red')\n\n# Simplify date formatting on x-axis\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\nax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=6))  # Show labels only for every nth day (change interval as needed)\n\n# Rotate date labels for better visibility\nplt.xticks(rotation=45)\n\n# Add x-axis label instead of individual dates\nplt.xlabel('Date')\n\nplt.show()","metadata":{},"outputs":[],"execution_count":null}]}